{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta notebook contiene bloques de código útiles para realizar Q-learning en el entorno \"CartPole-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiar render_mode a rgb_array para entrenar/testear\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretización de los estados\n",
    "\n",
    "**Nota:** es importante que chequeen el espacio de observación y el espacio de acción del entorno. Los números usados son ejemplos y pueden no ser correctos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.e-06,  1.e+02])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart_position = np.linspace(-0.000001, 100., 2)\n",
    "cart_velocity = np.linspace(-0.000001, 100., 2)\n",
    "pole_angle = np.linspace(-0.000001, 100., 2)\n",
    "pole_angular_velocity = np.linspace(-0.000001, 100., 2)\n",
    "cart_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener el estado a partir de la observación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(obs):\n",
    "    cart_pos = obs[0]\n",
    "    cart_vel = obs[1]\n",
    "    pole_ang = obs[2]\n",
    "    pole_ang_vel = obs[3]\n",
    "    cart_pos_idx = np.digitize(cart_pos, cart_position)\n",
    "    cart_vel_idx = np.digitize(cart_vel, cart_velocity)\n",
    "    pole_angle_idx = np.digitize(pole_ang, pole_angle)\n",
    "    pole_ang_vel_idx = np.digitize(pole_ang_vel, pole_angular_velocity)\n",
    "    return cart_pos_idx, cart_vel_idx, pole_angle_idx, pole_ang_vel_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.6401935   0.43055007  0.11140139 -1.1124599 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.int64(1), np.int64(1), np.int64(1), np.int64(0))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.observation_space.sample()\n",
    "print(obs)\n",
    "state = get_state(obs)\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicilización de la tabla Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[0., 0.],\n",
       "          [0., 0.]],\n",
       "\n",
       "         [[0., 0.],\n",
       "          [0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0.],\n",
       "          [0., 0.]],\n",
       "\n",
       "         [[0., 0.],\n",
       "          [0., 0.]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[0., 0.],\n",
       "          [0., 0.]],\n",
       "\n",
       "         [[0., 0.],\n",
       "          [0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0.],\n",
       "          [0., 0.]],\n",
       "\n",
       "         [[0., 0.],\n",
       "          [0., 0.]]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.zeros((len(cart_position), len(cart_velocity), len(pole_angle), len(pole_angular_velocity), env.action_space.n))\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtención de la acción a partir de la tabla Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(state, Q):\n",
    "    action = np.argmax(Q[state])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon-Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, Q, epsilon=0.1):\n",
    "    explore = np.random.binomial(1, epsilon)\n",
    "    if explore:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = optimal_policy(state, Q)\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo de episodio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00881185  0.04519531 -0.04260313 -0.01280498]\n",
      "action 0\n",
      "action 1\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 1\n",
      "action 1\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "total_reward 16.0\n",
      "steps 16\n"
     ]
    }
   ],
   "source": [
    "obs,_ = env.reset()\n",
    "print(obs)\n",
    "done = False\n",
    "total_reward = 0\n",
    "state = get_state(obs)\n",
    "steps = 0\n",
    "while not done:\n",
    "    steps += 1\n",
    "    \n",
    "    # Acción del modelo\n",
    "    action = epsilon_greedy_policy(state, Q, 0.5)\n",
    "    print('action', action)\n",
    "\n",
    "    obs, reward, done, _, _ = env.step(action)\n",
    "    next_state = get_state(obs)\n",
    "    \n",
    "   # Q[state][action_idx] = ... # Completar\n",
    "   \n",
    "   # Actualizar estado\n",
    "    state = next_state\n",
    "   \n",
    "    total_reward += reward\n",
    "\n",
    "    env.render()\n",
    "\n",
    "env.close() # Para cerrar la ventana, hay que crear el ambiente de nuevo si queremos correrlo otra vez   \n",
    "print('total_reward', total_reward)\n",
    "print('steps', steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cartpole-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
